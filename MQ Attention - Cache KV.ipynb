{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Modelos de lengua basados en transformers\n","\n","Modifica el código del modelo de lengua basado en transformers para que use multi-query attention sin implementar todavía una caché KV.\n","\n","Realiza un pequeño estudio sobre cómo afecta esto a la calidad del modelo; para\n","esto puedes medir qué probabilidad da el modelo a algunas frases similares a las del conjunto de entrenamiento.\n","\n","Adicionalmente, estudia cómo afecta a la calidad del modelo el uso de una caché KV que tendrás que implementar. Aunque sería deseable poder medir el impacto de ambas cosas en los tiempos de ejecución, no es necesario que lo hagas, ya que probablemente no puedas medirlo con precisión suficiente salvo que incrementes el tamaño de los datos de entrenamiento y el número de parámetros del modelo.\n","\n","Explica en tu respuesta las ideas básicas tanto de multi-query attention como de la caché KV."],"metadata":{"id":"LnUM1rG1LozG"}},{"cell_type":"markdown","source":["## Modelo Original"],"metadata":{"id":"Hrduikho8kfh"}},{"cell_type":"code","source":["%%capture\n","%pip install torch"],"metadata":{"id":"vPnjY1i-8kfk","executionInfo":{"status":"ok","timestamp":1716377335211,"user_tz":-120,"elapsed":80763,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Preparación del mini batch"],"metadata":{"id":"hTVzto288kfl"}},{"cell_type":"code","source":["import torch\n","import random\n","\n","def make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n","\n","    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokenized_corpus]\n","    n_tokens = len(token_indices)  # number of tokens in the corpus\n","    assert n_tokens >= max_len, f'Short corpus ({n_tokens} tokens), must be at least {max_len} tokens long'\n","\n","    while True:\n","        input_batch, output_batch = [], []\n","\n","        for _ in range(batch_size):\n","            start_index = random.randint(0, n_tokens - 1)  # random start\n","            end_index = start_index + max_len\n","            input_seq = token_indices[start_index:end_index]\n","            if end_index > n_tokens:\n","                input_seq += token_indices[:end_index - n_tokens]\n","\n","            # output is input shifted one token to the right:\n","            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n","\n","            input_batch.append(input_seq)\n","            output_batch.append(output_seq)\n","\n","        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n","        pass  # this line will be executed next time the function is called"],"metadata":{"id":"ljAwGFdA8kfl","executionInfo":{"status":"ok","timestamp":1716377379674,"user_tz":-120,"elapsed":3060,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Importacion del transformer"],"metadata":{"id":"KU5NJP0t4V18"}},{"cell_type":"code","source":["%%capture\n","import os\n","colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))  # running in Google Colab?\n","if not os.path.isfile('transformer.ipynb') and colab:\n","    %pip install wget\n","    !wget https://raw.githubusercontent.com/jaspock/me/main/docs/materials/transformers/assets/notebooks/transformer.ipynb\n","\n","%pip install nbformat\n","%run './transformer.ipynb'\n","\n","set_seed(42)"],"metadata":{"id":"7G9j-tsV4Xou","executionInfo":{"status":"ok","timestamp":1716377403463,"user_tz":-120,"elapsed":22924,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Preproceso del corpus"],"metadata":{"id":"hDNPQiOB8kfn"}},{"cell_type":"code","source":["# download Tiny Shakespeare dataset:\n","import urllib.request\n","url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","chars = 10000  # number of characters to keep\n","corpus = urllib.request.urlopen(url).read().decode(\"utf-8\")[:chars]\n","print(corpus[:100])\n","\n","word_list = list(set(corpus.split()))\n","word_index = {'[PAD]': 0, '[UNK]': 1}\n","special_tokens = len(word_index)\n","for i, w in enumerate(word_list):\n","    word_index[w] = i + special_tokens\n","index_word = {i: w for i, w in enumerate(word_index)}\n","vocab_size = len(word_index)\n","print(f\"vocab_size = {vocab_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377403827,"user_tz":-120,"elapsed":366,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"d65b7178-68c0-48f5-cb71-fd970e21bfc9","id":"idFXgG_f8kfp"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n","vocab_size = 862\n"]}]},{"cell_type":"markdown","source":["### Entrenamiento del modelo"],"metadata":{"id":"P6tpuO978kfp"}},{"cell_type":"code","source":["n_layer = 2\n","n_head = 2\n","n_embd =  64\n","embd_pdrop = 0.1\n","resid_pdrop = 0.1\n","attn_pdrop = 0.1\n","batch_size = 4\n","max_len = 32\n","training_steps = 1000\n","eval_steps = 100\n","lr = 0.001\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import math\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = DecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,\n","                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n","\n","model.train()\n","tokenized_corpus = corpus.split()\n","step = 0\n","\n","for inputs, outputs in make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n","    optimizer.zero_grad()\n","    logits = model(inputs)\n","    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1))\n","    if step % eval_steps == 0:\n","        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","    step = step + 1\n","    if (step==training_steps):\n","        break\n","\n","print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377429063,"user_tz":-120,"elapsed":25239,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"0d03f3d7-127f-436e-a60f-50b188a0f214","id":"A0XNGjuA8kfp"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.16M\n","Step [0/1000], loss: 6.7625, perplexity: 864.83\n","Step [100/1000], loss: 4.7342, perplexity: 113.78\n","Step [200/1000], loss: 3.5120, perplexity: 33.52\n","Step [300/1000], loss: 2.6646, perplexity: 14.36\n","Step [400/1000], loss: 2.4456, perplexity: 11.54\n","Step [500/1000], loss: 1.3665, perplexity: 3.92\n","Step [600/1000], loss: 0.9380, perplexity: 2.55\n","Step [700/1000], loss: 0.8752, perplexity: 2.40\n","Step [800/1000], loss: 0.6776, perplexity: 1.97\n","Step [900/1000], loss: 0.6829, perplexity: 1.98\n","Step [1000/1000], loss: 0.5262, perplexity: 1.69\n"]}]},{"cell_type":"markdown","source":["### Evaluación del modelo"],"metadata":{"id":"Si6YXn7z8kfq"}},{"cell_type":"code","source":["\n","def generate_text(model, prompt, word_index, index_word, max_len, device):\n","    words = prompt.split()\n","    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n","    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n","\n","    with torch.no_grad():\n","        for _ in range(max_len - len(input_ids)):\n","            output = model(input)\n","            last_token_logits = output[0, -1, :]\n","            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n","            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n","            predicted_word = index_word[predicted_id]\n","            words.append(predicted_word)\n","\n","    return ' '.join(words)\n","\n","model.eval()\n","prompt = \"O God, that robot is out of control! I tell you, friends, \"\n","generated_text = generate_text(model, prompt, word_index, index_word, max_len, device)\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377429063,"user_tz":-120,"elapsed":12,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"6dbf60ff-5ee4-4b83-f487-98459fc04689","id":"48A1yFC98kfr"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["O God, that robot is out of control! I tell you, friends, most charitable care for you If you'll bestow a small--of what you have heard it; But, since it serves my\n"]}]},{"cell_type":"markdown","source":["## Multi Query Attention sin caché KV."],"metadata":{"id":"gADuOJj2Mytk"}},{"cell_type":"markdown","source":["### Preparación del mini batch"],"metadata":{"id":"VkFPrfQ6Nx_N"}},{"cell_type":"code","source":["import torch\n","import random\n","\n","def make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n","\n","    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokenized_corpus]\n","    n_tokens = len(token_indices)  # number of tokens in the corpus\n","    assert n_tokens >= max_len, f'Short corpus ({n_tokens} tokens), must be at least {max_len} tokens long'\n","\n","    while True:\n","        input_batch, output_batch = [], []\n","\n","        for _ in range(batch_size):\n","            start_index = random.randint(0, n_tokens - 1)  # random start\n","            end_index = start_index + max_len\n","            input_seq = token_indices[start_index:end_index]\n","            if end_index > n_tokens:\n","                input_seq += token_indices[:end_index - n_tokens]\n","\n","            # output is input shifted one token to the right:\n","            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n","\n","            input_batch.append(input_seq)\n","            output_batch.append(output_seq)\n","\n","        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n","        pass  # this line will be executed next time the function is called"],"metadata":{"id":"Yg1F7Zd7N2aq","executionInfo":{"status":"ok","timestamp":1716377429063,"user_tz":-120,"elapsed":2,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Nuevos Metodos"],"metadata":{"id":"guPyEaRLxdlp"}},{"cell_type":"code","source":["class MultiQueryBlock(Block):\n","    def __init__(self, n_embd, n_head, attn_pdrop=0.1, resid_pdrop=0.1):\n","        super().__init__(n_embd, n_head, attn_pdrop, resid_pdrop)\n","        self.multiquery_attention = MultiQueryAttention(n_embd, n_head, attn_pdrop)\n","\n","    def forward(self, x, mask):\n","        x = self.multiquery_attention(x, mask)\n","        x = self.feed_forward(x)\n","        return x\n","\n","\n","class MultiQueryAttention(nn.Module):\n","    def __init__(self, n_embd, n_head, attn_pdrop=0.1):\n","        super().__init__()\n","        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n","        self.n_embd = n_embd\n","        self.n_head = n_head\n","        self.d_head = n_embd // n_head\n","\n","        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n","        self.attn_drop = nn.Dropout(attn_pdrop)\n","        self.resid_drop = nn.Dropout(attn_pdrop)\n","\n","    def forward(self, x, mask):\n","        B, T, C = x.size()\n","        qkv = self.c_attn(x).view(B, T, 3, self.n_head, self.d_head).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        q = q / (self.d_head ** 0.5)\n","\n","        scores = torch.einsum(\"bhld,bhkd->bhlk\", q, k)\n","        scores.masked_fill_(mask == 0, float(\"-inf\"))\n","        attn = F.softmax(scores, dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = torch.einsum(\"bhlk,bhld->bhld\", attn, v)\n","        x = x.permute(0, 3, 1, 2).contiguous().view(B, T, C)\n","        x = self.resid_drop(x)\n","        return x\n","\n","\n","class MultiQueryDecoderTransformer(AbstractTransformer):\n","    def __init__(self, n_embd, n_head, n_layer, vocab_size, max_len,\n","                 embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1):\n","        super().__init__(n_embd=n_embd, n_head=n_head, n_layer=n_layer, max_len=max_len, vocab_size=vocab_size,\n","                         embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n","        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        super()._init_weights()\n","\n","    def forward(self, inputs):\n","        B, T = inputs.size()\n","        device = inputs.device\n","        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1).bool()\n","        mask = mask.view(1, T, T)\n","        x = super().forward(inputs, mask)\n","        logits = self.lm_head(x)\n","\n","        return logits\n"],"metadata":{"id":"oYfS-16xxfXe","executionInfo":{"status":"ok","timestamp":1716377429063,"user_tz":-120,"elapsed":2,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Preproceso del corpus"],"metadata":{"id":"pHdPH83hN8It"}},{"cell_type":"code","source":["# download Tiny Shakespeare dataset:\n","import urllib.request\n","url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","chars = 10000  # number of characters to keep\n","corpus = urllib.request.urlopen(url).read().decode(\"utf-8\")[:chars]\n","print(corpus[:100])\n","\n","word_list = list(set(corpus.split()))\n","word_index = {'[PAD]': 0, '[UNK]': 1}\n","special_tokens = len(word_index)\n","for i, w in enumerate(word_list):\n","    word_index[w] = i + special_tokens\n","index_word = {i: w for i, w in enumerate(word_index)}\n","vocab_size = len(word_index)\n","print(f\"vocab_size = {vocab_size}\")"],"metadata":{"id":"p4NPI55SN9_x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377429408,"user_tz":-120,"elapsed":347,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"b752648a-f272-4d1c-b38d-bd39bb770cf6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n","vocab_size = 862\n"]}]},{"cell_type":"markdown","source":["### Entrenamiento del modelo"],"metadata":{"id":"0KEdRARvOBwe"}},{"cell_type":"code","source":["n_layer = 2\n","n_head = 2\n","n_embd =  64\n","embd_pdrop = 0.1\n","resid_pdrop = 0.1\n","attn_pdrop = 0.1\n","batch_size = 4\n","max_len = 32\n","training_steps = 1000\n","eval_steps = 100\n","lr = 0.001\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import math\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model2 = MultiQueryDecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,\n","                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n","model2.to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n","optimizer = optim.Adam(model2.parameters(), lr=lr)\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n","\n","model2.train()\n","tokenized_corpus = corpus.split()\n","step = 0\n","\n","for inputs, outputs in make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n","    optimizer.zero_grad()\n","    logits = model2(inputs)\n","    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1))\n","    if step % eval_steps == 0:\n","        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","    step = step + 1\n","    if (step==training_steps):\n","        break\n","\n","print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')"],"metadata":{"id":"vEuCAtTUOBOl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377451256,"user_tz":-120,"elapsed":21850,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"a07d7625-3200-489c-d946-5c5f85f187e5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.16M\n","Step [0/1000], loss: 6.7847, perplexity: 884.21\n","Step [100/1000], loss: 5.1770, perplexity: 177.16\n","Step [200/1000], loss: 3.5851, perplexity: 36.06\n","Step [300/1000], loss: 2.5506, perplexity: 12.82\n","Step [400/1000], loss: 2.0883, perplexity: 8.07\n","Step [500/1000], loss: 1.3878, perplexity: 4.01\n","Step [600/1000], loss: 1.3365, perplexity: 3.81\n","Step [700/1000], loss: 0.9014, perplexity: 2.46\n","Step [800/1000], loss: 0.9228, perplexity: 2.52\n","Step [900/1000], loss: 0.5535, perplexity: 1.74\n","Step [1000/1000], loss: 0.6283, perplexity: 1.87\n"]}]},{"cell_type":"markdown","source":["### Evaluación del modelo"],"metadata":{"id":"vMoPnxarOHLe"}},{"cell_type":"code","source":["\n","def generate_text(model, prompt, word_index, index_word, max_len, device):\n","    words = prompt.split()\n","    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n","    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n","\n","    with torch.no_grad():\n","        for _ in range(max_len - len(input_ids)):\n","            output = model(input)\n","            last_token_logits = output[0, -1, :]\n","            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n","            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n","            predicted_word = index_word[predicted_id]\n","            words.append(predicted_word)\n","\n","    return ' '.join(words)\n","\n","model2.eval()\n","prompt = \"O God, that robot is out of control! I tell you, friends, \"\n","generated_text = generate_text(model2, prompt, word_index, index_word, max_len, device)\n","print(generated_text)\n"],"metadata":{"id":"UExaR6dBOKFy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377451256,"user_tz":-120,"elapsed":12,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"dfcccbaa-b323-48c7-eb37-a110f84217ad"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["O God, that robot is out of control! I tell you, friends, most grave belly be content to give good report fort, but that he should find you lions, finds you hares;\n"]}]},{"cell_type":"markdown","source":["## Multi Query Attention con caché KV."],"metadata":{"id":"RoSdnnatM4VH"}},{"cell_type":"markdown","source":["### Preparación del mini batch"],"metadata":{"id":"3YvUz2g8OSfM"}},{"cell_type":"code","source":["import torch\n","import random\n","\n","def make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n","\n","    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokenized_corpus]\n","    n_tokens = len(token_indices)  # number of tokens in the corpus\n","    assert n_tokens >= max_len, f'Short corpus ({n_tokens} tokens), must be at least {max_len} tokens long'\n","\n","    while True:\n","        input_batch, output_batch = [], []\n","\n","        for _ in range(batch_size):\n","            start_index = random.randint(0, n_tokens - 1)  # random start\n","            end_index = start_index + max_len\n","            input_seq = token_indices[start_index:end_index]\n","            if end_index > n_tokens:\n","                input_seq += token_indices[:end_index - n_tokens]\n","\n","            # output is input shifted one token to the right:\n","            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n","\n","            input_batch.append(input_seq)\n","            output_batch.append(output_seq)\n","\n","        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n","        pass  # this line will be executed next time the function is called"],"metadata":{"id":"1PTTNAzdOSfM","executionInfo":{"status":"ok","timestamp":1716377451256,"user_tz":-120,"elapsed":10,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Nuevos Métodos"],"metadata":{"id":"OCmsg_TwzrSE"}},{"cell_type":"code","source":["class MultiQueryAttention(nn.Module):\n","    def __init__(self, n_embd, n_head, attn_pdrop=0.1):\n","        super().__init__()\n","        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n","        self.n_embd = n_embd\n","        self.n_head = n_head\n","        self.d_head = n_embd // n_head\n","\n","        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n","        self.attn_drop = nn.Dropout(attn_pdrop)\n","        self.resid_drop = nn.Dropout(attn_pdrop)\n","\n","        self.key_cache = None\n","        self.value_cache = None\n","\n","    def forward(self, x, mask):\n","        B, T, C = x.size()\n","        qkv = self.c_attn(x).view(B, T, 3, self.n_head, self.d_head).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        if self.key_cache is None and self.value_cache is None:\n","            self.key_cache = k\n","            self.value_cache = v\n","        else:\n","            k = torch.cat([self.key_cache, k], dim=-2)\n","            v = torch.cat([self.value_cache, v], dim=-2)\n","\n","            self.key_cache = k\n","            self.value_cache = v\n","\n","        q = q / (self.d_head ** 0.5)\n","\n","        scores = torch.einsum(\"bhld,bhkd->bhlk\", q, k)\n","        scores.masked_fill_(mask == 0, float(\"-inf\"))\n","        attn = F.softmax(scores, dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = torch.einsum(\"bhlk,bhld->bhld\", attn, v)\n","        x = x.permute(0, 3, 1, 2).contiguous().view(B, T, C)\n","        x = self.resid_drop(x)\n","        return x\n","\n","\n","class MultiQueryDecoderTransformer(AbstractTransformer):\n","    def __init__(self, n_embd, n_head, n_layer, vocab_size, max_len,\n","                 embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1):\n","        super().__init__(n_embd=n_embd, n_head=n_head, n_layer=n_layer, max_len=max_len, vocab_size=vocab_size,\n","                         embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n","        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        super()._init_weights()\n","\n","    def forward(self, inputs):\n","        B, T = inputs.size()\n","        device = inputs.device\n","        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1).bool()  # causal attention mask\n","        mask = mask.view(1, T, T)  # expand mask, (T, T) -> (1, T, T)\n","        x = super().forward(inputs, mask)\n","        logits = self.lm_head(x)\n","\n","        return logits"],"metadata":{"id":"6BiUTGp7ztYU","executionInfo":{"status":"ok","timestamp":1716377451256,"user_tz":-120,"elapsed":10,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Preproceso del corpus"],"metadata":{"id":"IS5WBdI-OSfN"}},{"cell_type":"code","source":["# download Tiny Shakespeare dataset:\n","import urllib.request\n","url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","chars = 10000  # number of characters to keep\n","corpus = urllib.request.urlopen(url).read().decode(\"utf-8\")[:chars]\n","print(corpus[:100])\n","\n","word_list = list(set(corpus.split()))\n","word_index = {'[PAD]': 0, '[UNK]': 1}\n","special_tokens = len(word_index)\n","for i, w in enumerate(word_list):\n","    word_index[w] = i + special_tokens\n","index_word = {i: w for i, w in enumerate(word_index)}\n","vocab_size = len(word_index)\n","print(f\"vocab_size = {vocab_size}\")"],"metadata":{"id":"38PqWTd3OSfN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377451598,"user_tz":-120,"elapsed":352,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"e6cc453b-de7b-4ee2-cca0-5dcabc7c8a91"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n","vocab_size = 862\n"]}]},{"cell_type":"markdown","source":["### Entrenamiento del modelo"],"metadata":{"id":"Qk2kXcj8OSfO"}},{"cell_type":"code","source":["n_layer = 2\n","n_head = 2\n","n_embd =  64\n","embd_pdrop = 0.1\n","resid_pdrop = 0.1\n","attn_pdrop = 0.1\n","batch_size = 4\n","max_len = 32\n","training_steps = 1000\n","eval_steps = 100\n","lr = 0.001\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import math\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model3 = MultiQueryDecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,\n","                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n","model3.to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n","optimizer = optim.Adam(model3.parameters(), lr=lr)\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n","\n","model3.train()\n","tokenized_corpus = corpus.split()\n","step = 0\n","\n","for inputs, outputs in make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n","    optimizer.zero_grad()\n","    logits = model3(inputs)\n","    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1))\n","    if step % eval_steps == 0:\n","        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","    step = step + 1\n","    if (step==training_steps):\n","        break\n","\n","print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')"],"metadata":{"id":"vgmjzkTiOSfO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377467045,"user_tz":-120,"elapsed":15449,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"2c594959-8707-4a72-a94f-66396aaffb10"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.16M\n","Step [0/1000], loss: 6.7531, perplexity: 856.75\n","Step [100/1000], loss: 4.9128, perplexity: 136.03\n","Step [200/1000], loss: 3.5589, perplexity: 35.12\n","Step [300/1000], loss: 2.5533, perplexity: 12.85\n","Step [400/1000], loss: 1.8998, perplexity: 6.68\n","Step [500/1000], loss: 1.3750, perplexity: 3.96\n","Step [600/1000], loss: 1.0287, perplexity: 2.80\n","Step [700/1000], loss: 0.9463, perplexity: 2.58\n","Step [800/1000], loss: 0.7983, perplexity: 2.22\n","Step [900/1000], loss: 0.7975, perplexity: 2.22\n","Step [1000/1000], loss: 0.6249, perplexity: 1.87\n"]}]},{"cell_type":"markdown","source":["### Evaluación del modelo"],"metadata":{"id":"NlaVcskJOSfO"}},{"cell_type":"code","source":["\n","def generate_text(model, prompt, word_index, index_word, max_len, device):\n","    words = prompt.split()\n","    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n","    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n","\n","    with torch.no_grad():\n","        for _ in range(max_len - len(input_ids)):\n","            output = model(input)\n","            last_token_logits = output[0, -1, :]\n","            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n","            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n","            predicted_word = index_word[predicted_id]\n","            words.append(predicted_word)\n","\n","    return ' '.join(words)\n","\n","model3.eval()\n","prompt = \"O God, that robot is out of control! I tell you, friends, \"\n","generated_text = generate_text(model3, prompt, word_index, index_word, max_len, device)\n","print(generated_text)\n"],"metadata":{"id":"XqC6Yo59OSfO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716377467045,"user_tz":-120,"elapsed":12,"user":{"displayName":"Alfonso Ruiz Martinez","userId":"11094218471305292491"}},"outputId":"96d79a88-8e8e-43de-e074-126fffa56739"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["O God, that robot is out of control! I tell you, friends, most grave belly was deliberate, Not rash like his accusers, and thus answer'd: 'True is it, my incorporate friends,' quoth\n"]}]},{"cell_type":"markdown","source":["## Conclusiones"],"metadata":{"id":"YwOI21RDwdgC"}},{"cell_type":"markdown","source":["Las ideas básicas para multi-query attention y para caché kv son:\n","\n","- Multi-query attention: es un algoritmo que es utilizado para mejorar la eficiencia del modelo sin reducir prácticamente su exactitud. Consiste en reducir o eliminar las cabezas h de los valores K y V. A cada cabeza del valor de consulta Q se le aplica la misma transformación K y V.\n","\n","- Caché KV: es una técnica utilizada para mejorar la eficiencia de los modelos de atención. Se realiza almacenando los vectores de clave valor en una caché después de calcularse por primera vez, para así acceder a ellos posteriormente sin tener que volver a calcularlos.\n","\n","Como se puede apreciar después del entrenamiento de cada modelo, los modelos multi-query attention sin y con caché KV no mejoran al original. Pero sí que mejoran su tiempo de entrenamiento. El modelo original tardó 25s, mientras que el modelo sin caché KV tardó 20s y el modelo con caché KV tardó 15s. Se puede apreciar una gran mejora en el tiempo sin llegar a afectar significativamente en el rendimiento del modelo."],"metadata":{"id":"DbORqAME77IW"}}]}