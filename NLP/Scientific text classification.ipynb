{"cells":[{"cell_type":"markdown","metadata":{"id":"Y_sCmnWG-476"},"source":["# Practica 2 Bloque 2 - Mineria de textos.\n","\n","## Ensemble de clasificación de textos cientificos."]},{"cell_type":"markdown","metadata":{"id":"8K_6J4P2_Ino"},"source":["## Instalación de paquetes e importación de las librerias necesarias"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:45:56.768518Z","iopub.status.busy":"2024-04-30T08:45:56.768217Z","iopub.status.idle":"2024-04-30T08:47:25.077708Z","shell.execute_reply":"2024-04-30T08:47:25.076576Z","shell.execute_reply.started":"2024-04-30T08:45:56.768493Z"},"trusted":true},"outputs":[],"source":["!pip install tensorflow==2.15.0\n","!pip install transformers==4.32.1\n","!pip install scikit-learn==1.4.1.post1\n","!python -m spacy download es_core_news_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:25.080778Z","iopub.status.busy":"2024-04-30T08:47:25.080003Z","iopub.status.idle":"2024-04-30T08:47:48.883906Z","shell.execute_reply":"2024-04-30T08:47:48.882894Z","shell.execute_reply.started":"2024-04-30T08:47:25.080741Z"},"id":"VLMBiniX_bMK","outputId":"8ef66335-4e23-4f01-ed1d-9285d29845ce","trusted":true},"outputs":[],"source":["import os\n","\n","#  para construir gráficas y realizar análisis exploratorio de los datos\n","import plotly.graph_objects as go\n","import plotly.figure_factory as ff\n","import plotly.express as px\n","\n","# para cargar datos y realizar pre-procesamiento básico\n","import pandas as pd\n","from collections import Counter\n","\n","# para pre-procesamiento del texto y extraer características\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from nltk.stem.snowball import EnglishStemmer\n","\n","# algoritmos de clasificación\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","\n","# para construir pipelines\n","from sklearn.pipeline import Pipeline\n","\n","# para evaluar los modelos \n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n","from sklearn.utils.multiclass import unique_labels\n","\n","# para guardar el modelo\n","import pickle\n","\n","\n","# Librerias utilizadas por mi para diferentes tareas, como la definición del pipeline, la importación de diferentes clasificadores y la creación de gráficos.\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import GaussianNB\n","import nltk\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","from sklearn.pipeline import FeatureUnion\n","from nltk.corpus import wordnet\n","import spacy\n","import es_core_news_sm\n","import matplotlib.pyplot as plt\n","\n","# para guardar el modelo\n","import pickle\n","import tensorflow as tf\n","import keras\n","\n","# para visualizar el modelo\n","from IPython.core.display import display\n","\n","# algoritmos de clasificación, tokenizadores, etc.\n","from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig, TFDistilBertMainLayer\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import preprocessing\n","from sklearn.tree import plot_tree\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["La siguiente función se ha creado para definir la semilla de aleatoriedad en algunos de los paquetes que se utilizan en la práctica, con el objetivo de obtener el mismo resultado en diferentes ejecuciones."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:48.885844Z","iopub.status.busy":"2024-04-30T08:47:48.885233Z","iopub.status.idle":"2024-04-30T08:47:49.019718Z","shell.execute_reply":"2024-04-30T08:47:49.018761Z","shell.execute_reply.started":"2024-04-30T08:47:48.885816Z"},"trusted":true},"outputs":[],"source":["def set_random_seed(seed_value):\n","    tf.random.set_seed(seed_value)\n","    spacy.util.fix_random_seed(seed_value)\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","    \n","set_random_seed(199)"]},{"cell_type":"markdown","metadata":{"id":"Xym3Ov-j_DsW"},"source":["## Obtencion de los Datasets"]},{"cell_type":"markdown","metadata":{},"source":["El conjunto de datos escogido para la realizacion de la practica se encuentra en Kaggle en el siguiente enlace: https://www.kaggle.com/datasets/vivmankar/physics-vs-chemistry-vs-biology.\n","\n","Se trata de una serie de textos cientificos junto a su categoria: textos sobre biologia, fisica y quimica.\n","\n","Para ello he decidido juntar los datasets de entrenamiento y validacion de Kaggle para realizar mis propias divisiones, a continuación se lee el archivo .CSV que contiene los textos y las categorias de estos."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.022121Z","iopub.status.busy":"2024-04-30T08:47:49.021800Z","iopub.status.idle":"2024-04-30T08:47:49.151167Z","shell.execute_reply":"2024-04-30T08:47:49.150355Z","shell.execute_reply.started":"2024-04-30T08:47:49.022096Z"},"id":"h3eCEve3_eYF","outputId":"7a4d38d3-30ff-4601-d568-15bcd32e5dd2","trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/datap2bloq2mintext/data.csv')"]},{"cell_type":"markdown","metadata":{"id":"1JNYHUWi_O0e"},"source":["## Analisis exploratorio del conjunto de datos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.152912Z","iopub.status.busy":"2024-04-30T08:47:49.152527Z","iopub.status.idle":"2024-04-30T08:47:49.182380Z","shell.execute_reply":"2024-04-30T08:47:49.181467Z","shell.execute_reply.started":"2024-04-30T08:47:49.152877Z"},"trusted":true},"outputs":[],"source":["data"]},{"cell_type":"markdown","metadata":{},"source":["Como se observa, se trata de un dataset simple, con la columna objetivo y la columna que dispone de los textos."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.183890Z","iopub.status.busy":"2024-04-30T08:47:49.183514Z","iopub.status.idle":"2024-04-30T08:47:49.430954Z","shell.execute_reply":"2024-04-30T08:47:49.429995Z","shell.execute_reply.started":"2024-04-30T08:47:49.183837Z"},"trusted":true},"outputs":[],"source":["text_col = 'Comment'  # columna del dataframe que contiene el texto (depende del formato de los datos)\n","class_col = 'Topic'  # columna del dataframe que contiene la clase (depende del formato de los datos)\n","\n","# obtener algunas estadísticas sobre los datos\n","categories = sorted(data[class_col].unique(), reverse=False)\n","hist= Counter(data[class_col]) \n","print(f'Total de instancias -> {data.shape[0]}')\n","print('Distribución de clases:')\n","for item in sorted(hist.items(), key=lambda x: x[0]): print(f'    {item[0]}: {round(item[1]/len(data[class_col]), 3)}')\n","\n","print(f'Categorías -> {categories}')\n","print(f'Comentario de ejemplo -> {data[text_col][0]}')\n","print(f'Categoría del comentario -> {data[class_col][0]}')\n","\n","fig = go.Figure(layout=go.Layout(height=400, width=600))\n","fig.add_trace(go.Bar(x=categories, y=[hist[cat] for cat in categories]))\n","fig.show()\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["En el grafico anterior se muesstra la distribución de las clases, se puede observar a simple vista que hay muchos mas textos sobre biologia y quimica que sobre fisica, por lo que se procederá a un undersampling de las clases mayoritarias, para obtener un numero identico de instancias de cada clase."]},{"cell_type":"markdown","metadata":{"id":"CLfqZ8D3_RZj"},"source":["## Preprocesamiento del Dataset"]},{"cell_type":"markdown","metadata":{},"source":["Para eliminar datos innecesarios, se procede a eliminar la columna 'id'."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.432373Z","iopub.status.busy":"2024-04-30T08:47:49.432092Z","iopub.status.idle":"2024-04-30T08:47:49.454138Z","shell.execute_reply":"2024-04-30T08:47:49.453236Z","shell.execute_reply.started":"2024-04-30T08:47:49.432348Z"},"trusted":true},"outputs":[],"source":["data.drop(columns='id', inplace=True)\n","\n","data[class_col].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["En el resultado de la ejecución anterior se listan el total de instancias de cada clase.\n","\n","En la siguiente celda de código se aplicará el $Undersampling$ a los textos sobre biologia y sobre quimica: para ello he escogido una muestra de tamaño $n=2650$ (para tener el mismo numero de instancias que los textos sobre fisica) del conjunto de datos, la he concatenado con los datos de fisica y finalmente he mezclado (shuffling).\n","\n","NOTA: Como el entrenamiento de los modelos requiere de mucho tiempo de ejecución, se ha decidido emplear únicamente 500 muestras de cada categoria.\n","\n","Finalmente se han guardado los datos preprocesados en una variable para poder utilizarlos posteriormente, sin tener que volver a realizar todo el preproceso de estos."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.457083Z","iopub.status.busy":"2024-04-30T08:47:49.456710Z","iopub.status.idle":"2024-04-30T08:47:49.483838Z","shell.execute_reply":"2024-04-30T08:47:49.483043Z","shell.execute_reply.started":"2024-04-30T08:47:49.457049Z"},"trusted":true},"outputs":[],"source":["\n","''' #UNDERSAMPLING\n","# Tamaño de la muestra\n","n = 2650\n","\n","# Muestra de tamaño n\n","muestra_bio = data[data[class_col] == 'Biology'].sample(n=n, random_state=1)\n","muestra_quim = data[data[class_col] == 'Chemistry'].sample(n=n, random_state=1)\n","\n","# Combinar los datos\n","data = pd.concat([data[data[class_col] == 'Physics'], muestra_bio, muestra_quim])\n","\n","# Mezclar los datos (shuffling)\n","data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Guardar los datos originales tras el preproceso para utilizarlos posteriormente\n","datos_procesados = data\n","\n","# Imprimir total de cada clase\n","data[class_col].value_counts()\n","\n","'''\n","\n","#MUESTRAS REDUCIDAS\n","n = 500\n","\n","# Muestra de tamaño n\n","muestra_bio = data[data[class_col] == 'Biology'].sample(n=n, random_state=1)\n","muestra_quim = data[data[class_col] == 'Chemistry'].sample(n=n, random_state=1)\n","muestra_fisica = data[data[class_col] == 'Physics'].sample(n=n, random_state=1)\n","\n","# Combinar las muestras de cada clase\n","data_muestra = pd.concat([muestra_bio, muestra_quim, muestra_fisica])\n","\n","# Mezclar los datos (shuffling)\n","data_muestra = data_muestra.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Guardar los datos de muestra preprocesados para utilizarlos posteriormente\n","data = data_muestra\n","\n","# Imprimir total de cada clase\n","data[class_col].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.485182Z","iopub.status.busy":"2024-04-30T08:47:49.484925Z","iopub.status.idle":"2024-04-30T08:47:49.498103Z","shell.execute_reply":"2024-04-30T08:47:49.497282Z","shell.execute_reply.started":"2024-04-30T08:47:49.485160Z"},"trusted":true},"outputs":[],"source":["categories = sorted(data[class_col].unique(), reverse=False)\n","hist= Counter(data[class_col]) \n","for item in sorted(hist.items(), key=lambda x: x[0]): print(f'    {item[0]}: {round(item[1]/len(data[class_col]), 3)}')\n","fig = go.Figure(layout=go.Layout(height=400, width=600))\n","fig.add_trace(go.Bar(x=categories, y=[hist[cat] for cat in categories]))\n","fig.show()\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["En este punto ya se dispone del conjunto de datos preparado para el entrenamiento de los diferentes modelos y la creación del ensemble."]},{"cell_type":"markdown","metadata":{},"source":["## Conjunto de datos de entrenamiento y validacion"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se divide el conjunto de datos en datos de entrenamiento y validación. En este caso he elegido realizar una división del 75% de los datos para el entrenamiento de los modelos y el 25% restante para su validación."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.503013Z","iopub.status.busy":"2024-04-30T08:47:49.502630Z","iopub.status.idle":"2024-04-30T08:47:49.512533Z","shell.execute_reply":"2024-04-30T08:47:49.511619Z","shell.execute_reply.started":"2024-04-30T08:47:49.502984Z"},"trusted":true},"outputs":[],"source":["seed = 0  # fijar random_state para reproducibilidad\n","train, val = train_test_split(data, test_size=.25, stratify=data[class_col], random_state=seed)\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["# Primer Modelo - Pipeline simple de clasificación"]},{"cell_type":"markdown","metadata":{},"source":["El primero de los 3 modelos que se crearán para la clasificación de textos cientificos es el visto al inicio de la asignatura, un pipeline simple de clasificación textual."]},{"cell_type":"markdown","metadata":{},"source":["### Funciones necesarias"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se muestran las funciones necesarias para la predicción y la evaluación del modelo, asi como otras funciones auxiliares y un listado de stopwords que se eliminarán del texto."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.514504Z","iopub.status.busy":"2024-04-30T08:47:49.513835Z","iopub.status.idle":"2024-04-30T08:47:49.536962Z","shell.execute_reply":"2024-04-30T08:47:49.536157Z","shell.execute_reply.started":"2024-04-30T08:47:49.514471Z"},"trusted":true},"outputs":[],"source":["#listado de stopwords. Este listado también se puede leer desde un fichero utilizando la función read_corpus\n","stop_words=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves',\n","            'he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their',\n","            'theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are',\n","            'was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an',\n","            'the','and','but','if','or','because','as','until','while','of','at','by','for','with','about',\n","            'against','between','into','through','during','before','after','above','below','to','from','up',\n","            'down','in','out','on','off','over','under','again','further','then','once','here','there','when',\n","            'where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor',\n","            'not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now', 'ever']\n","\n","\n","# función auxiliar. Se utiliza al obtener la representación mediante TF-IDF del texto pues en este caso\n","# se removerán las stop_words y se considerarán los \"stem\" en lugar de las palabrass\n","def english_stemmer(sentence):\n","    stemmer = EnglishStemmer()\n","    analyzer = CountVectorizer(binary=False, analyzer='word', stop_words=stop_words, ngram_range=(1, 1)).build_analyzer()\n","    return (stemmer.stem(word) for word in analyzer(sentence))\n","\n","\n","# guarda un pipeline entrenado\n","def save_model(model, modelName = \"pickle_model.pkl\"):\n","   pkl_filename = modelName\n","   with open(pkl_filename, 'wb') as file:\n","    pickle.dump(model, file)   \n","\n","\n","# carga un pipeline entrenado y guardado previamente\n","def load_model(rutaModelo = \"pickle_model.pkl\"):\n","  # Load from file\n","  with open(rutaModelo, 'rb') as file:\n","    pickle_model = pickle.load(file)\n","    return pickle_model \n","\n","\n","# función auxiliar para realizar predicciones con el modelo\n","def predict_model01(model, data, pref='m'):\n","  \"\"\"\n","  data: list of the text to predict\n","  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n","  \"\"\"\n","  res = {}\n","  scores = None\n","  labels = model.predict(data)\n","\n","  if hasattr(model, 'predict_proba'):\n","    scores = model.predict_proba(data)\n","  \n","    # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n","    res = {f'scores_{pref}_{cls.lower()}':score for cls, score in zip(model.classes_, [col for col in scores.T])}\n","\n","  # añadir datos relativos a la predicción\n","  res[f'labels_{pref}'] = labels\n","\n","  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabéticamente.\n","  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n","\n","  return res\n","\n","\n","# función auxiliar que evalúa los resultados de una clasificación\n","def evaluate_model01(y_true, y_pred, y_score=None, pos_label='positive'):\n","  \"\"\"\n","  \n","  \"\"\"\n","  print('==== Sumario de la clasificación ==== ')\n","  print(classification_report(y_true, y_pred))\n","\n","  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n","\n","  # graficar matriz de confusión\n","  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n","  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n","\n","  z = cm[::-1]\n","  x = display_labels\n","  y =  x[::-1].copy()\n","  z_text = [[str(y) for y in x] for x in z]\n","\n","  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n","\n","  fig_cm.update_layout(\n","      height=400, width=400,\n","      showlegend=True,\n","      margin={'t':150, 'l':0},\n","      title={'text' : 'Matriz de Confusión', 'x':0.5, 'y':0.95, 'xanchor': 'center'},\n","      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n","      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n","  )\n","  fig_cm.show()\n","\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["### Creación y entrenamiento del modelo "]},{"cell_type":"markdown","metadata":{},"source":["En este apartado es donde se va a entrenar el modelo de clasificación.\n","\n","En la siguiente porción de código se muestra el pipeline, se trata de un clasificador SVC, al que se le pasa la columna con el texto cientifico preprocesada.\n","\n","Para el preproceso de la columna textual únicamente se aplica un CountVectorizer, para convertir el texto en una matriz de tokens, y posteriormente la técnica TF-IDF, para determinar la importancia de las palabras dentro del texto en comparación con los demas textos."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.538237Z","iopub.status.busy":"2024-04-30T08:47:49.537977Z","iopub.status.idle":"2024-04-30T08:47:49.552438Z","shell.execute_reply":"2024-04-30T08:47:49.551569Z","shell.execute_reply.started":"2024-04-30T08:47:49.538215Z"},"trusted":true},"outputs":[],"source":["\n","\n","def preprocess_pipeline():\n","    text_transformer = Pipeline([\n","        ('vect', CountVectorizer(analyzer=english_stemmer)),\n","        ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True))\n","    ])\n","\n","    preprocessor = ColumnTransformer([\n","        ('text1', text_transformer, \"Comment\")\n","    ], remainder='passthrough')\n","\n","    classifier = SVC(probability=True)\n","\n","    model = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('classifier', classifier)\n","    ])\n","\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["Seguidamente, al modelo creado se le ajsuta el conjunto de datos de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:49.553889Z","iopub.status.busy":"2024-04-30T08:47:49.553323Z","iopub.status.idle":"2024-04-30T08:47:51.591377Z","shell.execute_reply":"2024-04-30T08:47:51.590381Z","shell.execute_reply.started":"2024-04-30T08:47:49.553841Z"},"trusted":true},"outputs":[],"source":["\n","\n","model01 = preprocess_pipeline()\n","model01.fit(train.loc[:, [\"Comment\"]], train['Topic'])"]},{"cell_type":"markdown","metadata":{},"source":["# Segundo modelo - Fine Tuning"]},{"cell_type":"markdown","metadata":{},"source":["El siguiente modelo trata de un reajuste de un modelo previamente entrenado, en este caso se trata de un modelo transoformers, en concreto el DistilBert. "]},{"cell_type":"markdown","metadata":{},"source":["### Funciones necesarias"]},{"cell_type":"markdown","metadata":{},"source":["De igual manera, se definen las diferentes funciones necesarias para el entrenamiento del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:51.593612Z","iopub.status.busy":"2024-04-30T08:47:51.593037Z","iopub.status.idle":"2024-04-30T08:47:51.619693Z","shell.execute_reply":"2024-04-30T08:47:51.618900Z","shell.execute_reply.started":"2024-04-30T08:47:51.593576Z"},"trusted":true},"outputs":[],"source":["# función auxiliar para obtener tensores de entrada al modelo a partir del texto\n","def get_model_inputs02(cfg, data):\n","  # obtener ids y máscaras para el conjunto de entrenamiento\n","  # no es necesario convertir a tensores porque la salida del tokenizador se encuentra en este formato,\n","  encodings = cfg['tokenizer'](data, truncation=True, padding='max_length', max_length=cfg['max_length'], return_tensors=cfg['framework'])\n","\n","\n","  # obtener representación tf-idf de cada instancia\n","  tfidf = cfg['vectorizer'].transform(data)\n","  tfidf_t = tf.convert_to_tensor(tfidf.toarray(), dtype='int32')\n","\n","  # formatear los datos (tensores) de entrada de acuerdo con las opciones permitidas por TensorFlow\n","  # los nombres de las capas de Input creadas al construir el modelo ('input_ids', 'attention_mask', 'tfidf')\n","  # son utilizados como llaves en los diccionarios que representan las entradas al modelo\n","  inputs = {'input_ids': encodings['input_ids'],\n","            'attention_mask': encodings['attention_mask'],\n","            'tfidf': tfidf_t\n","           }\n","\n","  return inputs\n","\n","\n","# función auxiliar para realizar predicciones con el modelo\n","def predict_model02(model, cfg, data, pref='m'):\n","  \"\"\"\n","  data: list of the text to predict\n","  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n","  \"\"\"\n","  res = {}\n","  inputs = get_model_inputs02(cfg, data)\n","  scores = model.predict(inputs)\n","\n","  # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n","  if cfg['num_labels']==1: # si es clasificación binaria, este modelo devuelve solo 1 score por instancia\n","    res = {f'scores_{pref}': scores[:,0]}\n","  else:\n","    res = {f'scores_{pref}_{cls.lower()}': score for cls, score in zip(cfg['label_binarizer'].classes_, [col for col in scores.T])}\n","\n","  # añadir datos relativos a la predicción\n","  labels = cfg['label_binarizer'].inverse_transform(scores)\n","  res[f'labels_{pref}'] = labels\n","\n","  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabéticamente\n","  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n","  return res\n","\n","\n","# función auxiliar que evalúa los resultados de una clasificación\n","def evaluate_model02(y_true, y_pred, y_score=None, pos_label='positive'):\n","  print('==== Sumario de la clasificación ==== ')\n","  print(classification_report(y_true, y_pred))\n","\n","  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n","\n","  # graficar matriz de confusión\n","  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n","  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n","\n","  z = cm[::-1]\n","  x = display_labels\n","  y =  x[::-1].copy()\n","  z_text = [[str(y) for y in x] for x in z]\n","\n","  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n","\n","  fig_cm.update_layout(\n","      height=400, width=400,\n","      showlegend=True,\n","      margin={'t':150, 'l':0},\n","      title={'text' : 'Matriz de Confusión', 'x':0.5, 'y':0.95, 'xanchor': 'center'},\n","      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n","      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n","  )\n","  fig_cm.show()\n","\n","def get_model_graph02(cfg):\n","  # cargar capa que representa al transformer, en este caso, TFDistilBertMainLayer\n","  transformer = TFDistilBertModel.from_pretrained(cfg['transformer_model_name'], return_dict=False).distilbert\n","\n","  # crear los 'placeholder' correspondientes a las entradas del modelo\n","  # crear variable que representará las entradas de id para el Transformer\n","  input_ids = keras.layers.Input(shape=(cfg['max_length'],), name='input_ids', dtype='int32')\n","\n","  # crear variable que representará las entradas de las máscaras para el Transformer\n","  input_masks = keras.layers.Input(shape=(cfg['max_length'],), name='attention_mask', dtype='int32')\n","\n","  # crear variable que representará las entradas correspondientes a los rasgos específicos de dominio.\n","  input_tfidf = keras.layers.Input(shape=(cfg['number_of_additional_features'],), name='tfidf', dtype='float32')\n","\n","  # indicar que TFDistilBertMainLayer se llama con input_ids e input_mask y capturar su salida, que contiene los embeddings correspondientes a cada token del texto\n","  # Existen varios criterios (ej. https://arxiv.org/pdf/1908.10084.pdf) sobre qué componenentes utilizar como rasgos,\n","  # en este caso, tomaremos el embedding correspondiente al token de inicio de texto [CLS] de modo similar a TFDistilBertForSequenceClassification\n","  transformer_output = transformer(input_ids, attention_mask=input_masks)\n","\n","  # extraer embedding del token [CLS]\n","  # la transformación dependerá del tipo de salida del Transformer utilizado, en este caso TFDistilBertMainLayer\n","  # cuya salida es una tupla de un único elemento, que contiene un arreglo de dimensiones\n","  # (number_of_instances, number_of_tokens, embedding_dimension), donde el token 0 corresponde al CLS.\n","  transformes_cls_embedding = keras.layers.Lambda(lambda seq: seq[0][:,0,:], name='lambda')(transformer_output)\n","\n","  # concatenar embedding del token [CLS] con el vector de rasgos adicionales.\n","  features = keras.layers.concatenate([transformes_cls_embedding, input_tfidf], name='concatenate')\n","\n","  # establecer algunos hiper-parámetros del modelo\n","  initializer_range = 0.02\n","  hiden_units = 768\n","  seq_classif_dropout=0.2\n","  initializer = keras.initializers.TruncatedNormal(stddev=initializer_range)\n","\n","  # crear pre_classifier, establecer como su entrada los rasgos concatenados (features).\n","  pre_classifier = keras.layers.Dense(hiden_units, kernel_initializer=initializer, activation='relu', name='pre_classifier')(features)\n","\n","  # crear dropout layer y establecer como su entrada la salida de pre_classifier.\n","  dropout_layer = keras.layers.Dropout(rate=seq_classif_dropout, name='dropout')(pre_classifier)\n","\n","  # crear classifier layer y establecer como su entrada la salida de la capa dropout.\n","  classifier = keras.layers.Dense(cfg['num_labels'], kernel_initializer=initializer, name='classifier')(dropout_layer)\n","\n","  return input_ids, input_masks, input_tfidf, classifier\n","\n","def configure_model02(input_ids, input_masks, input_tfidf, classifier):\n","  # definir algoritmo de optimización\n","  optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n","\n","  # definir función loss. Debe cuidarse que sea coherente con la salida esperada del modelo (vector de num_labels elementos)\n","  # y el formato de los ejemplos (vector one-hot de num_labels componentes para codificar las categorías)\n","  loss = keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","  # crear el modelo\n","  model = keras.Model(inputs=[input_ids, input_masks, input_tfidf], outputs=classifier, name='distilbert-custom') # conectar todos los nodos en un modelo\n","\n","  # compilar el modelo, indicando otras métricas que se desee monitorear\n","  # La métrica debe ser apropiada para el tipo de problema (clasificación binaria o multiclase)\n","  #model.compile(optimizer=optimizer, loss=loss, metrics=['binary_accuracy'])\n","  model.compile(optimizer=optimizer, loss=loss, metrics=['categorical_accuracy'])\n","\n","  return model\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["### Creación y entrenamiento del modelo "]},{"cell_type":"markdown","metadata":{},"source":["En la siguiente porción de código se muestran las configuraciones del modelo.\n","\n","En este caso se han adaptado el código de ejemplo proporcionado por el profesorado y se ham modificado los valores de las siguientes configuraciones:\n","- num_labels\n","- number_of_additional_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:51.621097Z","iopub.status.busy":"2024-04-30T08:47:51.620797Z","iopub.status.idle":"2024-04-30T08:47:51.635454Z","shell.execute_reply":"2024-04-30T08:47:51.634659Z","shell.execute_reply.started":"2024-04-30T08:47:51.621065Z"},"trusted":true},"outputs":[],"source":["cfg02 = {}  # diccionario para agrupar configuraciones y variables para su posterior uso\n","cfg02['framework'] = 'tf'  # TensorFlow como framework (por cuestiones del formato en los datos)\n","cfg02['max_length'] = 512  # máxima longitud de secuencia recomendada por DistilBERT\n","cfg02['transformer_model_name'] = 'distilbert-base-uncased'\n","cfg02['number_of_additional_features'] = 634  # específico al problema, en este caso, será la dimensión del vector tf-idf\n","\n","\n","cfg02['num_labels'] = 3  # cambiar este número según el número de clases\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["Una vez instanciadas las configuraciones necesarias, se define el modelo y se muestran por pantalla."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:47:51.636758Z","iopub.status.busy":"2024-04-30T08:47:51.636490Z","iopub.status.idle":"2024-04-30T08:48:02.160314Z","shell.execute_reply":"2024-04-30T08:48:02.159429Z","shell.execute_reply.started":"2024-04-30T08:47:51.636729Z"},"trusted":true},"outputs":[],"source":["input_ids, input_masks, input_tfidf, classifier = get_model_graph02(cfg02)\n","model02 = configure_model02(input_ids, input_masks, input_tfidf, classifier)\n","\n","# imprimir sumario del modelo\n","model02.summary()\n","\n","# graficar el modelo (opcional)\n","model_image = tf.keras.utils.plot_model(model02, show_shapes=True, show_layer_names=True)\n","display(model_image)\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["El siguiente paso es el de definir el tokenizador a utilizar, en este caso el proporcionado por el modelo transformer DistilBert, la vectorizacion TF-IDF, de la misma manera que para el primer modelo, y finalmente la función que convierte las etiquetas en vectores binarios."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:48:02.162315Z","iopub.status.busy":"2024-04-30T08:48:02.161940Z","iopub.status.idle":"2024-04-30T08:48:02.967611Z","shell.execute_reply":"2024-04-30T08:48:02.966712Z","shell.execute_reply.started":"2024-04-30T08:48:02.162282Z"},"trusted":true},"outputs":[],"source":["# cargar el tokenizador, disponible en Transformers\n","cfg02['tokenizer'] = DistilBertTokenizer.from_pretrained(cfg02['transformer_model_name'] )\n","\n","# instanciar TfidfVectorizer\n","cfg02['vectorizer'] = TfidfVectorizer(stop_words='english', max_features=cfg02['number_of_additional_features'])\n","\n","# instanciar y entrenar LabelBinarizer\n","cfg02['label_binarizer'] = preprocessing.LabelBinarizer() # guardar para su posterior uso al decodificar predicciones\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se entrenan las funciones TfidfVectorizer y LabelBinarizer, para ajustarlas a los datos de entrenamiento, y finalmente en la celda siguiente se ajusta y entrena el modelo transformers."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:48:02.969541Z","iopub.status.busy":"2024-04-30T08:48:02.968900Z","iopub.status.idle":"2024-04-30T08:48:05.447350Z","shell.execute_reply":"2024-04-30T08:48:05.446398Z","shell.execute_reply.started":"2024-04-30T08:48:02.969507Z"},"trusted":true},"outputs":[],"source":["# entrenar TfidfVectorizer\n","cfg02['vectorizer'].fit(train[text_col].to_list())\n","\n","# guardar TfidfVectorizer entrenado para su posterior uso (codificar nuevos datos).\n","with open('vectorizer_reviews.pkl', 'wb') as f:\n","    pickle.dump(cfg02['vectorizer'], f)\n","\n","# entrenar LabelBinarizer\n","cfg02['label_binarizer'].fit(train[class_col])\n","\n","# guardar LabelBinarizer para su uso posterior (decodificar las predicciones de nuevos datos)\n","with open('label_binarizer_reviews.pkl', 'wb') as f:\n","    pickle.dump(cfg02['label_binarizer'], f)\n","\n","# obtener codificación one-hot\n","train_blabels = cfg02['label_binarizer'].transform(train[class_col])\n","val_blabes = cfg02['label_binarizer'].transform(val[class_col])\n","\n","# obtener tensores correspondientes\n","train_blabels_t = tf.convert_to_tensor(train_blabels, dtype='int32')\n","val_blabels_t = tf.convert_to_tensor(val_blabes, dtype='int32')\n","\n","# obtener diccionarios representando las entradas del modelo\n","train_inputs = get_model_inputs02(cfg02, train[text_col].to_list())\n","val_inputs = get_model_inputs02(cfg02, val[text_col].to_list())\n","\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T08:48:05.449724Z","iopub.status.busy":"2024-04-30T08:48:05.448632Z","iopub.status.idle":"2024-04-30T09:00:42.266756Z","shell.execute_reply":"2024-04-30T09:00:42.265784Z","shell.execute_reply.started":"2024-04-30T08:48:05.449696Z"},"trusted":true},"outputs":[],"source":["# configuraciones\n","cfg02['checkpoints_dir'] = 'checkpoints'  # directorio donde se guardarán los checkpoints al entrenar el modelo\n","cfg02['model_name'] = 'distilbert-reviews'  # identificador al guardar los checkpoints\n","cfg02['trained_model_name'] = os.path.join(cfg02['checkpoints_dir'], cfg02['model_name'])\n","\n","epochs_max = 10\n","epochs_to_save = 1 # si epochs_max % epochs_to_save !=0 podrían realizarse iteraciones extras\n","batch_size = 16\n","\n","# ciclo de entrenamiento y guardar checkpoints\n","for epoch_current in range(0, epochs_max, epochs_to_save):\n","    epoch_from = epoch_current +1\n","    epoch_to = epoch_current + epochs_to_save\n","    print(f'Training model, epochs {epoch_from} - {epoch_to}')\n","\n","    # entrenar el modelo. Opcionalmente, se puede suministrar datos de validación => validation_data=(val_inputs,val_blabels_t )\n","    model02.fit(train_inputs, y=train_blabels_t, initial_epoch=epoch_current, epochs=epoch_to, batch_size=batch_size, validation_data=(val_inputs,val_blabels_t))\n","\n","    model02.save_weights(cfg02['trained_model_name'], save_format=\"tf\")\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["# Tercer modelo - Fine Tuning"]},{"cell_type":"markdown","metadata":{},"source":["Este último modelo entrenado es similar al anterior pero con ligeras modificaciones, como por ejemplo una capa dropout añadida para la regularización.\n","\n","Ademas también se ha modificado el parámetro del número adicional de carácteristicas, que cambiará el tamaño de las capas dentro del modelo, por lo tanto también sus predicciones."]},{"cell_type":"markdown","metadata":{},"source":["### Funciones necesarias"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se muestran las funciones necesarias del modelo, en este caso a diferencia de los anteriores, se ha creado una nueva función de configuración para añadir el dropout mencionado anteriormente."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:00:42.268867Z","iopub.status.busy":"2024-04-30T09:00:42.268566Z","iopub.status.idle":"2024-04-30T09:00:42.296890Z","shell.execute_reply":"2024-04-30T09:00:42.296014Z","shell.execute_reply.started":"2024-04-30T09:00:42.268830Z"},"trusted":true},"outputs":[],"source":["# función auxiliar para obtener tensores de entrada al modelo a partir del texto\n","def get_model_inputs03(cfg, data):\n","  # obtener ids y máscaras para el conjunto de entrenamiento\n","  # no es necesario convertir a tensores porque la salida del tokenizador se encuentra en este formato,\n","  encodings = cfg['tokenizer'](data, truncation=True, padding='max_length', max_length=cfg['max_length'], return_tensors=cfg['framework'])\n","\n","\n","  # obtener representación tf-idf de cada instancia\n","  tfidf = cfg['vectorizer'].transform(data)\n","  tfidf_t = tf.convert_to_tensor(tfidf.toarray(), dtype='int32')\n","\n","  # formatear los datos (tensores) de entrada de acuerdo con las opciones permitidas por TensorFlow\n","  # los nombres de las capas de Input creadas al construir el modelo ('input_ids', 'attention_mask', 'tfidf')\n","  # son utilizados como llaves en los diccionarios que representan las entradas al modelo\n","  inputs = {'input_ids': encodings['input_ids'],\n","            'attention_mask': encodings['attention_mask'],\n","            'tfidf': tfidf_t\n","           }\n","\n","  return inputs\n","\n","\n","# función auxiliar para realizar predicciones con el modelo\n","def predict_model03(model, cfg, data, pref='m'):\n","  \"\"\"\n","  data: list of the text to predict\n","  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n","  \"\"\"\n","  res = {}\n","  inputs = get_model_inputs03(cfg, data)\n","  scores = model.predict(inputs)\n","\n","  # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n","  if cfg['num_labels']==1: # si es clasificación binaria, este modelo devuelve solo 1 score por instancia\n","    res = {f'scores_{pref}': scores[:,0]}\n","  else:\n","    res = {f'scores_{pref}_{cls.lower()}': score for cls, score in zip(cfg['label_binarizer'].classes_, [col for col in scores.T])}\n","\n","  # añadir datos relativos a la predicción\n","  labels = cfg['label_binarizer'].inverse_transform(scores)\n","  res[f'labels_{pref}'] = labels\n","\n","  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabéticamente\n","  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n","  return res\n","\n","\n","# función auxiliar que evalúa los resultados de una clasificación\n","def evaluate_model03(y_true, y_pred, y_score=None, pos_label='positive'):\n","  print('==== Sumario de la clasificación ==== ')\n","  print(classification_report(y_true, y_pred))\n","\n","  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n","\n","  # graficar matriz de confusión\n","  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n","  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n","\n","  z = cm[::-1]\n","  x = display_labels\n","  y =  x[::-1].copy()\n","  z_text = [[str(y) for y in x] for x in z]\n","\n","  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n","\n","  fig_cm.update_layout(\n","      height=400, width=400,\n","      showlegend=True,\n","      margin={'t':150, 'l':0},\n","      title={'text' : 'Matriz de Confusión', 'x':0.5, 'y':0.95, 'xanchor': 'center'},\n","      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n","      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n","  )\n","  fig_cm.show()\n","\n","def get_model_graph03(cfg):\n","  # cargar capa que representa al transformer, en este caso, TFDistilBertMainLayer\n","  transformer = TFDistilBertModel.from_pretrained(cfg['transformer_model_name'], return_dict=False).distilbert\n","\n","  # crear los 'placeholder' correspondientes a las entradas del modelo\n","  # crear variable que representará las entradas de id para el Transformer\n","  input_ids = keras.layers.Input(shape=(cfg['max_length'],), name='input_ids', dtype='int32')\n","\n","  # crear variable que representará las entradas de las máscaras para el Transformer\n","  input_masks = keras.layers.Input(shape=(cfg['max_length'],), name='attention_mask', dtype='int32')\n","\n","  # crear variable que representará las entradas correspondientes a los rasgos específicos de dominio.\n","  input_tfidf = keras.layers.Input(shape=(cfg['number_of_additional_features'],), name='tfidf', dtype='float32')\n","\n","  # indicar que TFDistilBertMainLayer se llama con input_ids e input_mask y capturar su salida, que contiene los embeddings correspondientes a cada token del texto\n","  # Existen varios criterios (ej. https://arxiv.org/pdf/1908.10084.pdf) sobre qué componenentes utilizar como rasgos,\n","  # en este caso, tomaremos el embedding correspondiente al token de inicio de texto [CLS] de modo similar a TFDistilBertForSequenceClassification\n","  transformer_output = transformer(input_ids, attention_mask=input_masks)\n","\n","  # extraer embedding del token [CLS]\n","  # la transformación dependerá del tipo de salida del Transformer utilizado, en este caso TFDistilBertMainLayer\n","  # cuya salida es una tupla de un único elemento, que contiene un arreglo de dimensiones\n","  # (number_of_instances, number_of_tokens, embedding_dimension), donde el token 0 corresponde al CLS.\n","  transformes_cls_embedding = keras.layers.Lambda(lambda seq: seq[0][:,0,:], name='lambda')(transformer_output)\n","\n","  # concatenar embedding del token [CLS] con el vector de rasgos adicionales.\n","  features = keras.layers.concatenate([transformes_cls_embedding, input_tfidf], name='concatenate')\n","\n","  # establecer algunos hiper-parámetros del modelo\n","  initializer_range = 0.02\n","  hiden_units = 768\n","  seq_classif_dropout=0.2\n","  initializer = keras.initializers.TruncatedNormal(stddev=initializer_range)\n","\n","  # crear pre_classifier, establecer como su entrada los rasgos concatenados (features).\n","  pre_classifier = keras.layers.Dense(hiden_units, kernel_initializer=initializer, activation='relu', name='pre_classifier')(features)\n","\n","  # crear dropout layer y establecer como su entrada la salida de pre_classifier.\n","  dropout_layer = keras.layers.Dropout(rate=seq_classif_dropout, name='dropout')(pre_classifier)\n","\n","  # crear classifier layer y establecer como su entrada la salida de la capa dropout.\n","  classifier = keras.layers.Dense(cfg['num_labels'], kernel_initializer=initializer, name='classifier')(dropout_layer)\n","\n","  return input_ids, input_masks, input_tfidf, classifier\n","'''\n","def configure_model03(input_ids, input_masks, input_tfidf, classifier):\n","  # definir algoritmo de optimización\n","  optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n","\n","  # definir función loss. Debe cuidarse que sea coherente con la salida esperada del modelo (vector de num_labels elementos)\n","  # y el formato de los ejemplos (vector one-hot de num_labels componentes para codificar las categorías)\n","  loss = keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","  # crear el modelo\n","  model = keras.Model(inputs=[input_ids, input_masks, input_tfidf], outputs=classifier, name='distilbert-custom') # conectar todos los nodos en un modelo\n","\n","  # compilar el modelo, indicando otras métricas que se desee monitorear\n","  # La métrica debe ser apropiada para el tipo de problema (clasificación binaria o multiclase)\n","  model.compile(optimizer=optimizer, loss=loss, metrics=['categorical_accuracy'])\n","\n","  return model\n","'''\n","\n","from tensorflow.keras.layers import Dropout\n","\n","def configure_model03(input_ids, input_masks, input_tfidf, classifier):\n","    optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n","\n","    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","    model_input = [input_ids, input_masks, input_tfidf]\n","    x = classifier\n","    # Dropout del 30%\n","    x = Dropout(0.3)(x)\n","    outputs = keras.layers.Dense(cfg03['num_labels'], activation='softmax')(x)\n","    model = keras.Model(inputs=model_input, outputs=outputs, name='distilbert-custom')\n","\n","    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","\n","    return model\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["### Creación y entrenamiento del modelo "]},{"cell_type":"markdown","metadata":{},"source":["En este punto es donde aparece diferenciación con el modelo anterior, en este caso el valor del número de caracteristicas adicionales es 324, que como se verá al final, cambia el comportamiento y la arquitectura de parámetros."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:00:42.298178Z","iopub.status.busy":"2024-04-30T09:00:42.297919Z","iopub.status.idle":"2024-04-30T09:00:42.325161Z","shell.execute_reply":"2024-04-30T09:00:42.324402Z","shell.execute_reply.started":"2024-04-30T09:00:42.298157Z"},"trusted":true},"outputs":[],"source":["cfg03 = {}  # diccionario para agrupar configuraciones y variables para su posterior uso\n","cfg03['framework'] = 'tf'  # TensorFlow como framework (por cuestiones del formato en los datos)\n","cfg03['max_length'] = 512  # máxima longitud de secuencia recomendada por DistilBERT\n","cfg03['transformer_model_name'] = 'distilbert-base-uncased'\n","cfg03['number_of_additional_features'] = 324  # específico al problema, en este caso, será la dimensión del vector tf-idf\n","\n","\n","cfg03['num_labels'] = 3  # cambiar este número según el número de clases\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se grafica de manera visual el modelo de clasificación."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:00:42.326259Z","iopub.status.busy":"2024-04-30T09:00:42.326014Z","iopub.status.idle":"2024-04-30T09:00:44.499949Z","shell.execute_reply":"2024-04-30T09:00:44.499049Z","shell.execute_reply.started":"2024-04-30T09:00:42.326237Z"},"trusted":true},"outputs":[],"source":["input_ids, input_masks, input_tfidf, classifier = get_model_graph03(cfg03)\n","model03 = configure_model03(input_ids, input_masks, input_tfidf, classifier)\n","\n","# imprimir sumario del modelo\n","model03.summary()\n","\n","# graficar el modelo (opcional)\n","model_image = tf.keras.utils.plot_model(model03, show_shapes=True, show_layer_names=True)\n","display(model_image)\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["En el gráfico anterior se puede ver el nuevo modelo creado, con unas pequeñas diferencias que el modelo 02.\n","\n","A continuación se procede de manera identica, ajustando al modelo los datos de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:00:44.501550Z","iopub.status.busy":"2024-04-30T09:00:44.501267Z","iopub.status.idle":"2024-04-30T09:00:45.952871Z","shell.execute_reply":"2024-04-30T09:00:45.951900Z","shell.execute_reply.started":"2024-04-30T09:00:44.501526Z"},"trusted":true},"outputs":[],"source":["# cargar el tokenizador, disponible en Transformers\n","cfg03['tokenizer'] = DistilBertTokenizer.from_pretrained(cfg03['transformer_model_name'] )\n","\n","# instanciar TfidfVectorizer\n","cfg03['vectorizer'] = TfidfVectorizer(stop_words='english', max_features=cfg03['number_of_additional_features'])\n","\n","# instanciar y entrenar LabelBinarizer\n","cfg03['label_binarizer'] = preprocessing.LabelBinarizer() # guardar para su posterior uso al decodificar predicciones\n","\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:00:45.954634Z","iopub.status.busy":"2024-04-30T09:00:45.954259Z","iopub.status.idle":"2024-04-30T09:00:48.499244Z","shell.execute_reply":"2024-04-30T09:00:48.498223Z","shell.execute_reply.started":"2024-04-30T09:00:45.954601Z"},"trusted":true},"outputs":[],"source":["# entrenar TfidfVectorizer\n","cfg03['vectorizer'].fit(train[text_col].to_list())\n","\n","# guardar TfidfVectorizer entrenado para su posterior uso (codificar nuevos datos).\n","with open('vectorizer_reviews.pkl', 'wb') as f:\n","    pickle.dump(cfg03['vectorizer'], f)\n","\n","# entrenar LabelBinarizer\n","cfg03['label_binarizer'].fit(train[class_col])\n","\n","# guardar LabelBinarizer para su uso posterior (decodificar las predicciones de nuevos datos)\n","with open('label_binarizer_reviews.pkl', 'wb') as f:\n","    pickle.dump(cfg03['label_binarizer'], f)\n","\n","# obtener codificación one-hot\n","train_blabels = cfg03['label_binarizer'].transform(train[class_col])\n","val_blabes = cfg03['label_binarizer'].transform(val[class_col])\n","\n","# obtener tensores correspondientes\n","train_blabels_t = tf.convert_to_tensor(train_blabels, dtype='int32')\n","val_blabels_t = tf.convert_to_tensor(val_blabes, dtype='int32')\n","\n","# obtener diccionarios representando las entradas del modelo\n","train_inputs = get_model_inputs03(cfg03, train[text_col].to_list())\n","val_inputs = get_model_inputs03(cfg03, val[text_col].to_list())\n","\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:00:48.501352Z","iopub.status.busy":"2024-04-30T09:00:48.500697Z","iopub.status.idle":"2024-04-30T09:13:05.145790Z","shell.execute_reply":"2024-04-30T09:13:05.144765Z","shell.execute_reply.started":"2024-04-30T09:00:48.501317Z"},"trusted":true},"outputs":[],"source":["# configuraciones\n","cfg03['checkpoints_dir'] = 'checkpoints'  # directorio donde se guardarán los checkpoints al entrenar el modelo\n","cfg03['model_name'] = 'distilbert-reviews'  # identificador al guardar los checkpoints\n","cfg03['trained_model_name'] = os.path.join(cfg03['checkpoints_dir'], cfg03['model_name'])\n","\n","epochs_max = 10\n","epochs_to_save = 1 # si epochs_max % epochs_to_save !=0 podrían realizarse iteraciones extras\n","batch_size = 16\n","\n","# ciclo de entrenamiento y guardar checkpoints\n","for epoch_current in range(0, epochs_max, epochs_to_save):\n","    epoch_from = epoch_current +1\n","    epoch_to = epoch_current + epochs_to_save\n","    print(f'Training model, epochs {epoch_from} - {epoch_to}')\n","\n","    # entrenar el modelo. Opcionalmente, se puede suministrar datos de validación => validation_data=(val_inputs,val_blabels_t )\n","    model03.fit(train_inputs, y=train_blabels_t, initial_epoch=epoch_current, epochs=epoch_to, batch_size=batch_size, validation_data=(val_inputs,val_blabels_t))\n","\n","    model03.save_weights(cfg03['trained_model_name'], save_format=\"tf\")\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["# Construcción del Ensemble"]},{"cell_type":"markdown","metadata":{},"source":["En este punto ya se disponen de 3 modelos de clasificación textual entrenados, por lo que el siguiente y último paso es el de crear el ensemble de los modelos y entrenarlo mediante los datos de entrenamiento.\n","\n","Se va a seguir la estrategia de construcción manual del ensemble, la empleada por el profesorado en las notebooks proporcionadas.\n","\n","Por lo que el primer paso es el de hacer predicciones sobre cada uno de los 3 modelos, para asi obtener las probabilidades de pertenencia a cada una de las 3 clases."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:05.147426Z","iopub.status.busy":"2024-04-30T09:13:05.147115Z","iopub.status.idle":"2024-04-30T09:13:05.151432Z","shell.execute_reply":"2024-04-30T09:13:05.150606Z","shell.execute_reply.started":"2024-04-30T09:13:05.147384Z"},"trusted":true},"outputs":[],"source":["data = train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:05.152833Z","iopub.status.busy":"2024-04-30T09:13:05.152572Z","iopub.status.idle":"2024-04-30T09:13:06.960159Z","shell.execute_reply":"2024-04-30T09:13:06.959124Z","shell.execute_reply.started":"2024-04-30T09:13:05.152811Z"},"trusted":true},"outputs":[],"source":["m01_pred = predict_model01(model01, data[[text_col]], pref='m01')\n","\n","print(f'\\n {m01_pred.head(5)}')\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:06.965213Z","iopub.status.busy":"2024-04-30T09:13:06.964944Z","iopub.status.idle":"2024-04-30T09:13:32.533083Z","shell.execute_reply":"2024-04-30T09:13:32.532170Z","shell.execute_reply.started":"2024-04-30T09:13:06.965191Z"},"trusted":true},"outputs":[],"source":["m02_pred = predict_model02(model02, cfg02, data[text_col].to_list(), pref='m02')\n","print(f'\\n {m02_pred.head(5)}')\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:32.534701Z","iopub.status.busy":"2024-04-30T09:13:32.534339Z","iopub.status.idle":"2024-04-30T09:13:58.181065Z","shell.execute_reply":"2024-04-30T09:13:58.180172Z","shell.execute_reply.started":"2024-04-30T09:13:32.534668Z"},"trusted":true},"outputs":[],"source":["m03_pred = predict_model03(model03, cfg03, data[text_col].to_list(), pref='m03')\n","print(f'\\n {m03_pred.head(5)}')\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["Una vez obtenidas todas las probabilidades, se van a concatenar en la matriz cmb, a la que posteriormente se eliminarán las columnas con las etiquetas para permanecer unicamente con las probabilidades obtenidas por cada modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:58.182549Z","iopub.status.busy":"2024-04-30T09:13:58.182244Z","iopub.status.idle":"2024-04-30T09:13:58.210561Z","shell.execute_reply":"2024-04-30T09:13:58.209495Z","shell.execute_reply.started":"2024-04-30T09:13:58.182523Z"},"trusted":true},"outputs":[],"source":["cmb = pd.concat([m01_pred, m02_pred, m03_pred], axis=1)\n","cmb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:58.212537Z","iopub.status.busy":"2024-04-30T09:13:58.211945Z","iopub.status.idle":"2024-04-30T09:13:58.230432Z","shell.execute_reply":"2024-04-30T09:13:58.229336Z","shell.execute_reply.started":"2024-04-30T09:13:58.212505Z"},"trusted":true},"outputs":[],"source":["cmb.drop(columns=['labels_m01', 'labels_m02', 'labels_m03'], inplace=True)\n","\n","X = cmb.values\n","y = data[class_col] # !ATENCIÓN! el orden de las instancias debe ser el mismo en X e y\n","\n","print(cmb.head(5))\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["En este punto ya disponemos en la variable X con las probabilidades de cada clase y con la variable y en la que se almacenan las categorias reales de los textos.\n","\n","El siguiente paso es el de entrenar el clasificador con los datos generados, en este caso al igual que las notebooks proporcionadas en la asignatura, se ha entrenado un arbol de decision que dispone de la ventaja de interpretabilidad al dibujar el arbol."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:58.231696Z","iopub.status.busy":"2024-04-30T09:13:58.231414Z","iopub.status.idle":"2024-04-30T09:13:59.178689Z","shell.execute_reply":"2024-04-30T09:13:59.177785Z","shell.execute_reply.started":"2024-04-30T09:13:58.231663Z"},"trusted":true},"outputs":[],"source":["# instanciar el clasificador\n","classifier = DecisionTreeClassifier(criterion='entropy', random_state=seed)\n","\n","# entrenar el clasificador\n","classifier.fit(X, y)\n","\n","# graficar el árbol\n","fig = plt.figure(figsize=(20,20))\n","\n","plot_tree(classifier, feature_names=cmb.columns, class_names=classifier.classes_, filled=True, impurity=False)\n","plt.show()\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["El árbol generado permite interpretar el funcionamiento del ensemble, la ecuación úbicada en la parte superior de cada nodo permite analizar las diferentes divisiones hacia los nodos hijo y las diferentes clases que se predicen.\n","\n","Como se observa al principio utiliza principalmente los scores del modelo 02, esto permite predecir mejor los textos que hablan sobre biologia, posteriormente en las ramas inferiores utiliza el modelo 03 para predecir textos quimicos y el 02 para predecir textos sobre fisica."]},{"cell_type":"markdown","metadata":{},"source":["# Evaluacion del ensemble y de los 3 modelos"]},{"cell_type":"markdown","metadata":{},"source":["En este punto ya se disponen de los tres modelos entrenados y del ensemble de clasificación creado. Por lo que el siguiente paso es analizar y comparar sus resultados sobre el conjunto de validación."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:59.180434Z","iopub.status.busy":"2024-04-30T09:13:59.179999Z","iopub.status.idle":"2024-04-30T09:13:59.184546Z","shell.execute_reply":"2024-04-30T09:13:59.183586Z","shell.execute_reply.started":"2024-04-30T09:13:59.180402Z"},"trusted":true},"outputs":[],"source":["data = val"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluación del primer modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:59.185791Z","iopub.status.busy":"2024-04-30T09:13:59.185544Z","iopub.status.idle":"2024-04-30T09:13:59.985194Z","shell.execute_reply":"2024-04-30T09:13:59.984281Z","shell.execute_reply.started":"2024-04-30T09:13:59.185769Z"},"trusted":true},"outputs":[],"source":["# predecir y evaluar conjunto de validación con el modelo 1\n","true_labels = data[class_col]\n","\n","m01_pred = predict_model01(model01, data[[\"Comment\"]], pref='m01')\n","if 'scores_m_positive' not in m01_pred:\n","    m01_pred['scores_m_positive'] = 0\n","\n","\n","# el nombre de los campos dependerá de pref al llamar a predic_model y las clases. Ver comentarios en la definición de la función\n","evaluate_model01(true_labels, m01_pred['labels_m01'], m01_pred['scores_m_positive'], 'positive')\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluación del segundo modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:13:59.986618Z","iopub.status.busy":"2024-04-30T09:13:59.986338Z","iopub.status.idle":"2024-04-30T09:14:08.002355Z","shell.execute_reply":"2024-04-30T09:14:08.001442Z","shell.execute_reply.started":"2024-04-30T09:13:59.986594Z"},"trusted":true},"outputs":[],"source":["true_labels = data[class_col]\n","\n","m02_pred = predict_model02(model02, cfg02, data[text_col].to_list(), pref='m02')\n","\n","evaluate_model02(true_labels, m02_pred['labels_m02'])  # notar que en este caso se no suministran los scores\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluación del tercer modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:14:08.004121Z","iopub.status.busy":"2024-04-30T09:14:08.003676Z","iopub.status.idle":"2024-04-30T09:14:15.945658Z","shell.execute_reply":"2024-04-30T09:14:15.944797Z","shell.execute_reply.started":"2024-04-30T09:14:08.004085Z"},"trusted":true},"outputs":[],"source":["true_labels = data[class_col]\n","\n","m03_pred = predict_model03(model03, cfg03, data[text_col].to_list(), pref='m03')\n","\n","evaluate_model03(true_labels, m03_pred['labels_m03'])  # notar que en este caso se no suministran los scores\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluacion del Ensemble"]},{"cell_type":"markdown","metadata":{},"source":["### Función necesaria"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se muestra la función necesaria para la validación del ensemble de modelos."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:14:15.947340Z","iopub.status.busy":"2024-04-30T09:14:15.946972Z","iopub.status.idle":"2024-04-30T09:14:15.956211Z","shell.execute_reply":"2024-04-30T09:14:15.955396Z","shell.execute_reply.started":"2024-04-30T09:14:15.947307Z"},"trusted":true},"outputs":[],"source":["# función auxiliar que evalúa los resultados de una clasificación\n","def evaluate_model(y_true, y_pred, y_score=None, pos_label='positive'):\n","  \"\"\"\n","  data: list of the text to predict\n","  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n","  \"\"\"\n","  print('==== Sumario de la clasificación ==== ')\n","  print(classification_report(y_true, y_pred))\n","\n","  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n","\n","  # graficar matriz de confusión\n","  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n","  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n","\n","  z = cm[::-1]\n","  x = display_labels\n","  y =  x[::-1].copy()\n","  z_text = [[str(y) for y in x] for x in z]\n","\n","  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n","\n","  fig_cm.update_layout(\n","      height=400, width=400,\n","      showlegend=True,\n","      margin={'t':150, 'l':0},\n","      title={'text' : 'Matriz de Confusión', 'x':0.5, 'y':0.95, 'xanchor': 'center'},\n","      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n","      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n","  )\n","  fig_cm.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluación"]},{"cell_type":"markdown","metadata":{},"source":["Para la evaluación del ensemble, es necesario proceder de la misma manera que para su creación y entrenamiento.\n","\n","Por lo que a continuación se crea nuevamente la matriz X con las probabilidades calculadas sobre el conjunto de datos de validación y la matriz y que dispone de las etiquetas reales de los textos."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:14:15.957598Z","iopub.status.busy":"2024-04-30T09:14:15.957340Z","iopub.status.idle":"2024-04-30T09:14:15.984476Z","shell.execute_reply":"2024-04-30T09:14:15.983619Z","shell.execute_reply.started":"2024-04-30T09:14:15.957576Z"},"trusted":true},"outputs":[],"source":["# combinar predicciones de los clasificadores base\n","cmb = pd.concat([m01_pred, m02_pred, m03_pred], axis=1)\n","cmb.drop(columns=['labels_m01', 'labels_m02', 'labels_m03', 'scores_m_positive'], inplace=True)\n","\n","X = cmb.values\n","y = data[class_col]  # !ATENCIÓN! el orden de las instancias debe ser el mismo en X e y\n","\n","print(cmb.head(5))\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T09:14:15.985780Z","iopub.status.busy":"2024-04-30T09:14:15.985469Z","iopub.status.idle":"2024-04-30T09:14:16.044749Z","shell.execute_reply":"2024-04-30T09:14:16.043909Z","shell.execute_reply.started":"2024-04-30T09:14:15.985743Z"},"trusted":true},"outputs":[],"source":["# predecir y evaluar conjunto de validación con el modelo 3\n","true_labels = y\n","\n","mc_pred = predict_model01(classifier, X, pref='mc')\n","\n","evaluate_model(true_labels, mc_pred['labels_mc'])\n","\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["# Resultados y Conclusiones"]},{"cell_type":"markdown","metadata":{},"source":["En la siguiente tabla se muestran los resultados de Accuracy obtenidos por los tres modelos.\n","\n","| Modelo | Accuracy |\n","|--------|----------|\n","| 01     | 65.60    |\n","| 02     | 69.33    |\n","| 03     | 66.93    |\n","\n","Mientras que el ensemble obtiene un **70.93** de acierto en las predicciones, por lo que el ensemble creado utilizado los 3 modelos es mas preciso en cuanto a la predicción de la clase de los textos cientificos.\n","\n","Al analizar las matrices de confusión tambien se observa las predicciones que realizan los modelos, por ejemplo, el modelo03 tiende a tener mejores predicciones sobre los textos quimicos, mientras que los demás modelos y el ensemble presentan unas predicciones mas similares en cada clase.\n","\n","Como último aspecto interesante a comentar, al realizar diferentes ejecuciones con diferentes números de instancias para cada clase, se ha observado que a medida que el conjunto de datos crece, el porcentaje de acierto de cada modelo disminuye. Esto podria indicar que los modelos generados tienden al sobreentrenamiento ya que presentan malos resultados sobre el conjunto de validación."]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4891125,"sourceId":8244514,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
